{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PUseM_N42in"
      },
      "source": [
        "이 노트북은 [케라스 창시자에게 배우는 딥러닝 2판](https://tensorflow.blog/kerasdl2/)의 예제 코드를 담고 있습니다.\n",
        "\n",
        "<table align=\"left\">\n",
        "    <tr>\n",
        "        <td>\n",
        "            <a href=\"https://colab.research.google.com/github/rickiepark/deep-learning-with-python-2nd/blob/main/chapter11_part03_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhOmioaR42iq"
      },
      "source": [
        "## 트랜스포머 아키텍처"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWSmzK5142ir"
      },
      "source": [
        "### 셀프 어텐션 이해하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XdzrCS-42ir"
      },
      "source": [
        "#### 일반화된 셀프 어텐션: 쿼리-키-값 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo8cK-c_42ir"
      },
      "source": [
        "### 멀티 헤드 어텐션"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0HqiH6C42ir"
      },
      "source": [
        "### 트랜스포머 인코더"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjYMcwAl42is"
      },
      "source": [
        "**데이터 가져오기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7VVn1yl42is",
        "outputId": "edfcf035-e3b2-4f70-e03b-5a0e192bb411"
      },
      "outputs": [],
      "source": [
        "# !rm -r aclImdb\n",
        "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "# !tar -xf aclImdb_v1.tar.gz\n",
        "# !rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8aiOAjJ42iu"
      },
      "source": [
        "**데이터 준비**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C8nBL2242iu",
        "outputId": "e536b586-ba3b-4be8-dab7-5aacd04dd0f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5244 files belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-06-10 01:21:04.879114: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-06-10 01:21:04.907109: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-06-10 01:21:04.907142: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-06-10 01:21:04.909704: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-06-10 01:21:04.909735: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-06-10 01:21:04.909749: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-06-10 01:21:05.013089: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-06-10 01:21:05.013140: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-06-10 01:21:05.013147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
            "2024-06-10 01:21:05.013171: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-06-10 01:21:05.020494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21458 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 19756 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    if not os.path.exists(val_dir / category):\n",
        "        os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0MsIvhr42iv"
      },
      "source": [
        "**데이터 벡터화**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "--rJ5PYw42iv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-06-10 01:21:45.566101: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZrJUai142iv"
      },
      "source": [
        "**`Layer` 층을 상속하여 구현한 트랜스포머 인코더**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "924jztFe42iv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yaSXFTO42iw"
      },
      "source": [
        "**트랜스포머 인코더를 사용하여 텍스트 분류하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nk9lx8A42iw",
        "outputId": "4aa30402-4915-4011-a165-293782cdb2fc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">543,776</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m5,120,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m543,776\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,664,033</span> (21.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,664,033\u001b[0m (21.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,664,033</span> (21.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,664,033\u001b[0m (21.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nXp84h_42iw"
      },
      "source": [
        "**트랜스포머 인코더 기반 모델 훈련하고 평가하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3H26gQC42ix",
        "outputId": "aa448162-347e-4a68-862b-a212504b542f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1717982507.203491   62210 service.cc:145] XLA service 0x7f62f403fe80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1717982507.203539   62210 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
            "2024-06-10 01:21:47.450620: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2024-06-10 01:21:47.876578: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8906\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1717982509.299676   62827 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_84', 188 bytes spill stores, 156 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982509.428329   62832 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_80', 8 bytes spill stores, 8 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982509.436760   62824 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_85', 204 bytes spill stores, 200 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982509.618413   62841 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982509.833434   62839 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_85', 116 bytes spill stores, 116 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982509.867262   62837 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982509.978998   62835 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_82', 200 bytes spill stores, 200 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982510.170826   62838 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_81', 480 bytes spill stores, 484 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982510.195325   62834 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_81', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982510.227223   62832 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_85', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982510.400410   62831 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_85', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982510.481867   62827 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_81', 12 bytes spill stores, 12 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982510.481910   62840 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_84', 668 bytes spill stores, 640 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982510.666609   62844 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_85', 736 bytes spill stores, 696 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982510.772262   62841 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_81', 344 bytes spill stores, 344 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982510.834009   62833 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_11', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982510.855597   62837 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_81', 108 bytes spill stores, 108 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982510.934121   62843 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_81', 240 bytes spill stores, 224 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982510.973941   62831 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_84', 72 bytes spill stores, 72 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982511.040821   62826 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_84', 136 bytes spill stores, 136 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m  2/164\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 85ms/step - accuracy: 0.5391 - loss: 2.1686"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1717982513.823930   62210 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m163/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.4878 - loss: 1.2097"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1717982530.643509   63243 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982530.732939   63232 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_81', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982530.792802   63249 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982530.794454   63250 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982530.794480   63244 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 12 bytes spill stores, 12 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982530.802221   63242 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982530.809368   63246 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982530.987113   63242 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_85', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982531.254596   63231 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_81', 480 bytes spill stores, 484 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982531.329037   63225 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_82', 200 bytes spill stores, 200 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982531.604190   63235 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_26', 432 bytes spill stores, 432 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982531.969884   63252 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_26', 12 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982532.070318   63231 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_85', 736 bytes spill stores, 696 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982532.164593   63243 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_84', 668 bytes spill stores, 640 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.4878 - loss: 1.2078"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1717982543.334558   63531 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982543.519520   63524 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982543.701048   63518 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982543.745021   63502 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 12 bytes spill stores, 12 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982543.877421   63521 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 240 bytes spill stores, 224 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982543.906424   63517 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 12 bytes spill stores, 12 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982543.920519   63528 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982543.944152   63509 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_8', 8 bytes spill stores, 8 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982543.957707   63522 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982543.973176   63525 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 108 bytes spill stores, 108 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982543.978249   63518 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982544.033454   63520 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 344 bytes spill stores, 344 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982544.125917   63529 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 480 bytes spill stores, 484 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 192ms/step - accuracy: 0.4878 - loss: 1.2059 - val_accuracy: 0.5007 - val_loss: 0.7266\n",
            "Epoch 2/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 134ms/step - accuracy: 0.5212 - loss: 0.7142 - val_accuracy: 0.7125 - val_loss: 0.6202\n",
            "Epoch 3/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.6606 - loss: 0.6060 - val_accuracy: 0.7854 - val_loss: 0.4522\n",
            "Epoch 4/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 131ms/step - accuracy: 0.7872 - loss: 0.4474 - val_accuracy: 0.8215 - val_loss: 0.3952\n",
            "Epoch 5/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.8226 - loss: 0.3944 - val_accuracy: 0.8229 - val_loss: 0.3915\n",
            "Epoch 6/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.8658 - loss: 0.3116 - val_accuracy: 0.8274 - val_loss: 0.3862\n",
            "Epoch 7/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 133ms/step - accuracy: 0.8851 - loss: 0.2772 - val_accuracy: 0.8380 - val_loss: 0.3850\n",
            "Epoch 8/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 130ms/step - accuracy: 0.9005 - loss: 0.2321 - val_accuracy: 0.7777 - val_loss: 0.4985\n",
            "Epoch 9/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 130ms/step - accuracy: 0.9205 - loss: 0.1909 - val_accuracy: 0.8378 - val_loss: 0.3980\n",
            "Epoch 10/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 130ms/step - accuracy: 0.9362 - loss: 0.1655 - val_accuracy: 0.8423 - val_loss: 0.3927\n",
            "Epoch 11/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9482 - loss: 0.1393 - val_accuracy: 0.8492 - val_loss: 0.4038\n",
            "Epoch 12/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9476 - loss: 0.1249 - val_accuracy: 0.8480 - val_loss: 0.4143\n",
            "Epoch 13/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9595 - loss: 0.1060 - val_accuracy: 0.8316 - val_loss: 0.4688\n",
            "Epoch 14/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 123ms/step - accuracy: 0.9714 - loss: 0.0770 - val_accuracy: 0.8367 - val_loss: 0.4858\n",
            "Epoch 15/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9771 - loss: 0.0702 - val_accuracy: 0.8427 - val_loss: 0.4633\n",
            "Epoch 16/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9781 - loss: 0.0594 - val_accuracy: 0.8425 - val_loss: 0.5151\n",
            "Epoch 17/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 114ms/step - accuracy: 0.9894 - loss: 0.0376 - val_accuracy: 0.8369 - val_loss: 0.5448\n",
            "Epoch 18/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.9807 - loss: 0.0473 - val_accuracy: 0.8445 - val_loss: 0.5405\n",
            "Epoch 19/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 110ms/step - accuracy: 0.9824 - loss: 0.0479 - val_accuracy: 0.8389 - val_loss: 0.5909\n",
            "Epoch 20/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 108ms/step - accuracy: 0.9911 - loss: 0.0301 - val_accuracy: 0.8422 - val_loss: 0.5377\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'transformer_encoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m777/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8374 - loss: 0.3811"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1717982967.467528   68721 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_11', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982967.803138   68743 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 8 bytes spill stores, 8 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982967.853141   68747 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982967.903734   68735 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982967.945888   68741 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_11', 108 bytes spill stores, 108 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982967.956230   68748 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982967.982787   68740 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_11', 240 bytes spill stores, 224 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982968.021537   68742 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_11', 344 bytes spill stores, 344 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982968.030718   68723 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_11', 12 bytes spill stores, 12 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982968.039735   68736 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982968.045441   68749 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "I0000 00:00:1717982968.208763   68721 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_11', 480 bytes spill stores, 484 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.8374 - loss: 0.3811\n",
            "테스트 정확도: 0.837\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffb0_42j42ix"
      },
      "source": [
        "#### 위치 인코딩을 사용해 위치 정보 주입하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3eCKYxO42ix"
      },
      "source": [
        "**서브클래싱으로 위치 임베딩 구현하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7ngZXecb42ix"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    # def compute_mask(self, inputs, mask=None):\n",
        "    #     return tf.math.not_equal(inputs, 0)\n",
        "def compute_mask(self, inputs, mask=None):\n",
        "    return tf.keras.backend.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAOg6EBF42ix"
      },
      "source": [
        "#### 텍스트 분류 트랜스포머"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RXcbNDa42iy"
      },
      "source": [
        "**트랜스포머 인코더와 위치 임베딩 합치기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmzXmO7042iy",
        "outputId": "672c3025-15f0-47ea-ea17-3cd329269ac6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ positional_embedding_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,273,600</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)           │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">543,776</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ positional_embedding_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m5,273,600\u001b[0m │\n",
              "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)           │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m543,776\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,817,633</span> (22.19 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,817,633\u001b[0m (22.19 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,817,633</span> (22.19 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,817,633\u001b[0m (22.19 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 107ms/step - accuracy: 0.5419 - loss: 1.1295 - val_accuracy: 0.6746 - val_loss: 0.5955\n",
            "Epoch 2/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 81ms/step - accuracy: 0.7045 - loss: 0.5707 - val_accuracy: 0.7401 - val_loss: 0.5041\n",
            "Epoch 3/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.7978 - loss: 0.4274 - val_accuracy: 0.7760 - val_loss: 0.4570\n",
            "Epoch 4/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 82ms/step - accuracy: 0.8466 - loss: 0.3476 - val_accuracy: 0.8170 - val_loss: 0.4044\n",
            "Epoch 5/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 80ms/step - accuracy: 0.8744 - loss: 0.2980 - val_accuracy: 0.8148 - val_loss: 0.4095\n",
            "Epoch 6/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 81ms/step - accuracy: 0.9086 - loss: 0.2357 - val_accuracy: 0.8207 - val_loss: 0.3963\n",
            "Epoch 7/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 81ms/step - accuracy: 0.9253 - loss: 0.1941 - val_accuracy: 0.8095 - val_loss: 0.4526\n",
            "Epoch 8/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 80ms/step - accuracy: 0.9417 - loss: 0.1535 - val_accuracy: 0.8235 - val_loss: 0.4102\n",
            "Epoch 9/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.9612 - loss: 0.1119 - val_accuracy: 0.8216 - val_loss: 0.4392\n",
            "Epoch 10/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.9720 - loss: 0.0831 - val_accuracy: 0.8129 - val_loss: 0.4795\n",
            "Epoch 11/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.9775 - loss: 0.0666 - val_accuracy: 0.8190 - val_loss: 0.4738\n",
            "Epoch 12/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 76ms/step - accuracy: 0.9828 - loss: 0.0560 - val_accuracy: 0.8149 - val_loss: 0.5413\n",
            "Epoch 13/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.9872 - loss: 0.0392 - val_accuracy: 0.8155 - val_loss: 0.5601\n",
            "Epoch 14/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 76ms/step - accuracy: 0.9887 - loss: 0.0329 - val_accuracy: 0.8183 - val_loss: 0.5824\n",
            "Epoch 15/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 76ms/step - accuracy: 0.9929 - loss: 0.0229 - val_accuracy: 0.8165 - val_loss: 0.5942\n",
            "Epoch 16/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 74ms/step - accuracy: 0.9948 - loss: 0.0183 - val_accuracy: 0.8107 - val_loss: 0.7043\n",
            "Epoch 17/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9936 - loss: 0.0209 - val_accuracy: 0.8080 - val_loss: 0.6745\n",
            "Epoch 18/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.9964 - loss: 0.0143 - val_accuracy: 0.8084 - val_loss: 0.7676\n",
            "Epoch 19/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 69ms/step - accuracy: 0.9970 - loss: 0.0126 - val_accuracy: 0.8078 - val_loss: 0.7517\n",
            "Epoch 20/20\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - accuracy: 0.9962 - loss: 0.0106 - val_accuracy: 0.8072 - val_loss: 0.7947\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'positional_embedding_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/data/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'transformer_encoder_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.8285 - loss: 0.3837\n",
            "테스트 정확도: 0.825\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "# inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T17kNue242iy"
      },
      "source": [
        "### BoW 모델 대신 언제 시퀀스 모델을 사용하나요?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "chapter11_part03_transformer.i",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
