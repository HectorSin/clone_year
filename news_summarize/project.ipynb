{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 요약하기\n",
    "\n",
    "1. Abstractive 모델 구성을 위한 텍스트 전처리 단계가 체계적으로 진행되었다.\t분석단계, 정제단계, 정규화와 불용어 제거, 데이터셋 분리, 인코딩 과정이 빠짐없이 체계적으로 진행되었다.\n",
    "2. 텍스트 요약모델이 성공적으로 학습되었음을 확인하였다.\t모델 학습이 진행되면서 train loss와 validation loss가 감소하는 경향을 그래프를 통해 확인했으며, 실제 요약문에 있는 핵심 단어들이 요약 문장 안에 포함되었다.\n",
    "3. Extractive 요약을 시도해 보고 Abstractive 요약 결과과 함께 비교해 보았다.\t두 요약 결과를 문법완성도 측면과 핵심단어 포함 측면으로 나누어 비교하고 분석 결과를 표로 정리하여 제시하였다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           headlines  \\\n",
      "0  upGrad learner switches to career in ML & Al w...   \n",
      "1  Delhi techie wins free food from Swiggy for on...   \n",
      "2  New Zealand end Rohit Sharma-led India's 12-ma...   \n",
      "3  Aegon life iTerm insurance plan helps customer...   \n",
      "4  Have known Hirani for yrs, what if MeToo claim...   \n",
      "\n",
      "                                                text  \n",
      "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n",
      "1  Kunal Shah's credit card bill payment platform...  \n",
      "2  New Zealand defeated India by 8 wickets in the...  \n",
      "3  With Aegon Life iTerm Insurance plan, customer...  \n",
      "4  Speaking about the sexual harassment allegatio...  \n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "raw_data = pd.read_csv('data/news_summary_more.csv', encoding='iso-8859-1')\n",
    "print(raw_data.head())\n",
    "\n",
    "data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "텍스트 정제 (Cleaning)\n",
    "- HTML 태그, URL, 특수문자 제거\n",
    "- 불필요한 공백 제거\n",
    "- 이모지 제거\n",
    "\n",
    "토큰화 (Tokenization) : 텍스트를 단어 또는 문장 단위로 분리\n",
    "\n",
    "불용어 제거 (Stopwords Removal) : 의미가 적은 단어 제거 (예: a, an, the 등)\n",
    "\n",
    "소문자 변환 (Lowercasing) : 모든 단어를 소문자로 변환하여 일관성 유지\n",
    "\n",
    "어간 추출 및 표제어 추출 (Stemming and Lemmatization) : 단어의 기본 형태로 변환하여 단어 수 축소\n",
    "\n",
    "문장 길이 조정 (Sentence Length Adjustment) : 모델에 입력할 수 있는 텍스트 길이를 조정\n",
    "\n",
    "구두점 제거 (Punctuation Removal):구두점을 제거하여 텍스트 단순화\n",
    "\n",
    "특수문자 제거 (Special Characters Removal):텍스트에 포함된 특수문자 제거\n",
    "\n",
    "정규 표현식 (Regular Expressions): 특정 패턴을 찾아 변환 또는 제거\n",
    "\n",
    "단어 임베딩 (Word Embeddings):단어를 임베딩하여 벡터화 (예: Word2Vec, GloVe 등)\n",
    "\n",
    "패딩 (Padding):문장의 길이를 동일하게 맞추기 위해 패딩을 추가 (주로 최대 문장 길이에 맞춰 패딩 추가)\n",
    "\n",
    "어텐션 마스크 (Attention Mask):패딩된 부분을 어텐션 메커니즘에서 무시하기 위해 마스크를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98401\n",
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 98360\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 98280\n",
      "headlines    0\n",
      "text         0\n",
      "dtype: int64\n",
      "전체 샘플수 : 98360\n",
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 98360\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 98262\n",
      "headlines    0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 중복, null값 확인 및 제거 \n",
    "def find_duplicated_null(data):\n",
    "    print('전체 샘플수 :', (len(data)))\n",
    "    print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['text'].nunique())\n",
    "    print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['headlines'].nunique())\n",
    "    print(data.isnull().sum())\n",
    "    \n",
    "    data.drop_duplicates(subset = ['text'], inplace=True) # text 열 기준 중복 제거\n",
    "    data.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = find_duplicated_null(data)\n",
    "data = find_duplicated_null(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 정규화(text normalization)와 불용어 제거\n",
    "# 같은 의미인데도 다른 표현으로 쓰여 마치 다른 단어들처럼 간주되는 경우 (예를 들면 it'll은 it will과 같고, mustn't은 must not과 같음)\n",
    "\n",
    "# 정규화 사전 생성\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "# 불용어 :  텍스트에는 자주 등장하지만 자연어 처리를 할 때 실질적으로 별 도움이 되지 않는 단어들 (예를 들면, I, my, me, over, 조사, 접미사 등, 의미를 분석해도 도움이 되지 않는 단어들)\n",
    "# 학습 데이터에서 빈도수가 낮은 단어나 길이가 짧은 단어들을 제거하는 것이 효과적일 수 있음\n",
    "# 타겟 데이터에서는 지우지 않는게 자연스러운 문장이 됨 \n",
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  everything bought great infact ordered twice third ordered wasfor mother father\n",
      "summary: great way to start the day\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\<.*?\\>', '', sentence) # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub(r'http\\S+|www\\S+|https\\S+', '', sentence, flags=re.MULTILINE)  # URL 제거\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()  # 불필요한 공백 제거\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))  # 구두점 제거\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens\n",
    "\n",
    "# 확인\n",
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(\"text: \", preprocess_sentence(temp_text))\n",
    "print(\"summary:\", preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...   \n",
      "1  Kunal Shah's credit card bill payment platform...   \n",
      "2  New Zealand defeated India by 8 wickets in the...   \n",
      "3  With Aegon Life iTerm Insurance plan, customer...   \n",
      "4  Speaking about the sexual harassment allegatio...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  saurav kant an alumnus of upgrad and iiitbs pg...  \n",
      "1  kunal shahs credit card bill payment platform ...  \n",
      "2  new zealand defeated india by  wickets in the ...  \n",
      "3  with aegon life iterm insurance plan customers...  \n",
      "4  speaking about the sexual harassment allegatio...  \n",
      "                                           headlines  \\\n",
      "0  upGrad learner switches to career in ML & Al w...   \n",
      "1  Delhi techie wins free food from Swiggy for on...   \n",
      "2  New Zealand end Rohit Sharma-led India's 12-ma...   \n",
      "3  Aegon life iTerm insurance plan helps customer...   \n",
      "4  Have known Hirani for yrs, what if MeToo claim...   \n",
      "\n",
      "                                   cleaned_headlines  \n",
      "0  upgrad learner switches to career in ml  al wi...  \n",
      "1  delhi techie wins free food from swiggy for on...  \n",
      "2  new zealand end rohit sharmaled indias match w...  \n",
      "3  aegon life iterm insurance plan helps customer...  \n",
      "4  have known hirani for yrs what if metoo claims...  \n"
     ]
    }
   ],
   "source": [
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # 소문자 변환\n",
    "    text = re.sub(r'\\<.*?\\>', '', text)  # HTML 태그 제거\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # URL 제거\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # 불필요한 공백 제거\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # 괄호로 묶인 내용 제거\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # 구두점 제거\n",
    "    text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "    return text\n",
    "\n",
    "# 텍스트 정제 적용\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "data['cleaned_headlines'] = data['headlines'].apply(clean_text)\n",
    "\n",
    "# 결과 확인\n",
    "print(data[['text', 'cleaned_text']].head())\n",
    "print(data[['headlines', 'cleaned_headlines']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
